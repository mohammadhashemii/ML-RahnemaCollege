# Week-2: Optimization & Machine Learning Fundamentals


## TODO

At the beginning of this week, we learn some base rules and theories of optimization and then we dive into the ML fundamentals.

Table of contents
==============

<!--ts-->
   * [Optimization](#optimization)
      * [Gradient Descent](#gradient-descent)
      * [Hassain Matrix](#hassain-matrix)
      * [Lagrange](#lagrange)
      * [KKT](#kkt)

<!--te-->

Optimization
==============
Gradient Descent
--------------

Algorithm: Is based on the observation that the most changes of function *f(x1, x2, ..., xn)* happen in the direction of the gradient:

![](https://github.com/mohammadhashemii/ML-RahnemaCollege/blob/master/Week-2/images/01_gradient-descent1.png)

Recall that the directional derivative of a function in direction of a unit vector *u* is defined by 

![](https://github.com/mohammadhashemii/ML-RahnemaCollege/blob/master/Week-2/images/02_gradient-descent2.png)

![](https://github.com/mohammadhashemii/ML-RahnemaCollege/blob/master/Week-2/images/03_gradient-descent3.png)
